# APPENDIX F  
SURVEY INSTRUMENT

This appendix contains the survey instruments used to gather data for the ENGAGIUM study. The surveys are divided into pre-development (baseline) and post-development (evaluation) phases.

---

## F.1 Pre-Development Survey Questionnaire

**Title:** Faculty Survey on Online Class Participation Tracking Practices, Technology Acceptance, and Usability Expectations

**Target Respondents:** Professors at St. Clare College of Caloocan

**Purpose:** To gather baseline data validating the problem, feasibility, and expected usability of ENGAGIUM prior to system development.

---

### SECTION A — Respondent Profile (Demographic Information)

*(Multiple choice / short answer)*

1. **Department**
   - SHS
   - College of Computer Studies
   - Other (please specify): _______________

2. **Years of Teaching Experience**
   - Less than 1 year
   - 1–3 years
   - 4–6 years
   - 7–10 years
   - More than 10 years

3. **In a typical day, how many online classes do you conduct?**
   - 1-3
   - 4-6
   - 7-10
   - More than 10

4. **In a typical week, how many online classes do you conduct?**
   - Less than 5
   - 5–10
   - 11–15
   - 16–20
   - 21–25
   - 26 or more

5. **Device Used Primarily When Teaching Online**
   - Laptop
   - Desktop Computer
   - Tablet
   - Mobile Phone
   - Others (please specify): _______________

---

### SECTION 1 — Problem Validation: Current Practices and Difficulties

*(5-Point Likert Scale: 1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4 = Agree, 5 = Strongly Agree)*

| Code | Item |
|------|------|
| PV1 | I find it difficult to monitor all forms of student participation during online classes. |
| PV2 | I often fail to notice quiet students who participate through chat. |
| PV3 | It is challenging to track microphone participation while lecturing. |
| PV4 | It is easy to miss student reactions (e.g., raise hand, thumbs up, emojis). |
| PV5 | Manual participation tracking affects my focus while teaching. |
| PV6 | Recording participation manually takes significant time and effort. |
| PV7 | My participation grading may sometimes appear subjective or inconsistent. |
| PV8 | Students sometimes question how participation grades were determined. |
| PV9 | I believe there is a need for a more structured and objective way to track participation. |
| PV10 | I find it challenging to maintain consistent participation records across multiple sessions. |

---

### SECTION 2 — Technology Acceptance & Feasibility (TAM-Based)

*(5-Point Likert Scale: 1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4 = Agree, 5 = Strongly Agree)*

#### 2.1 Perceived Usefulness (PU)

| Code | Item |
|------|------|
| PU1 | A tool that automatically tracks participation would make grading easier. |
| PU2 | Automated participation tracking would increase fairness in evaluating students. |
| PU3 | A system like ENGAGIUM would help ensure that all forms of participation are acknowledged. |
| PU4 | Automated analytics would help me better understand class engagement trends. |
| PU5 | A tracking system would reduce errors caused by manual monitoring. |

#### 2.2 Perceived Ease of Use (PEOU)

| Code | Item |
|------|------|
| PEOU1 | I am comfortable using browser-based tools or extensions. |
| PEOU2 | I can easily learn new digital platforms when needed. |
| PEOU3 | A dashboard for participation analytics would be easy for me to understand. |
| PEOU4 | I prefer tools that require minimal setup or configuration. |
| PEOU5 | I believe I can use an automated tracking tool without extensive training. |

#### 2.3 Behavioral Intention to Use (BI)

| Code | Item |
|------|------|
| BI1 | I am willing to use a participation-tracking tool if it is accurate. |
| BI2 | I will adopt a tool like ENGAGIUM if it reduces manual workload. |
| BI3 | I am likely to integrate automated tracking into my online class workflow. |
| BI4 | I am open to adopting new instructional technologies that improve teaching efficiency. |
| BI5 | I am willing to regularly use a tool like ENGAGIUM if it proves reliable. |

#### 2.4 Feasibility Constraints (FC)

| Code | Item |
|------|------|
| FC1 | My device is capable of running additional browser extensions. |
| FC2 | My internet connection is generally stable enough for online tools. |
| FC3 | I do not foresee major barriers in using an automated tracking system. |
| FC4 | I have access to updated browsers or software needed for such tools. |
| FC5 | I have sufficient familiarity with online teaching platforms to integrate an additional tool. |

---

### SECTION 3 — Usability Expectations for ENGAGIUM

*(5-Point Likert Scale: 1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4 = Agree, 5 = Strongly Agree)*

#### 3.1 Dashboard & Interface Expectations (DIE)

| Code | Item |
|------|------|
| DIE1 | I prefer a simple and clean dashboard layout. |
| DIE2 | I want the dashboard to show participation summaries at a glance. |
| DIE3 | I prefer having graphs and charts to visualize engagement. |
| DIE4 | I want the system to allow filtering by date, class, or participation type. |
| DIE5 | I want real-time indicators showing which participation types are active. |

#### 3.2 Report Preferences (RP)

| Code | Item |
|------|------|
| RP1 | I prefer downloadable participation reports (e.g., PDF, CSV). |
| RP2 | I want reports to contain totals, frequencies, and participation types. |
| RP3 | I prefer reports that highlight the top participants. |
| RP4 | I prefer reports that identify students who rarely participate. |
| RP5 | I want participation reports to show session-by-session engagement comparisons. |

#### 3.3 Tracking Options (TO)

| Code | Item |
|------|------|
| TO1 | I want the ability to enable or disable specific participation types. |
| TO2 | I prefer having customizable weighting for speaking, chat, and reactions. |
| TO3 | I want the option to manually adjust participation scores if needed. |
| TO4 | I expect the system to notify me when tracking starts or stops. |
| TO5 | I want the system to allow correction or deletion of incorrect participation logs. |

---

### SECTION 4 — Data Privacy and Ethical Expectations (DP)

*(5-Point Likert Scale: 1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4 = Agree, 5 = Strongly Agree)*

| Code | Item |
|------|------|
| DP1 | I expect the system to store only necessary participation data. |
| DP2 | I need assurance that no audio or video recordings will be stored. |
| DP3 | I expect clear transparency on how participation is scored and interpreted. |
| DP4 | I consider privacy compliance (e.g., Data Privacy Act of 2012) important for adopting such a system. |
| DP5 | I expect the system to include clear consent procedures before collecting participation data. |

---

### SECTION 5 — Open-Ended Questions

*(Short answer)*

1. What is your biggest challenge in tracking student participation in online classes?

   _______________________________________________________________________

2. What features would make an automated participation tracker most helpful for you?

   _______________________________________________________________________

3. Do you have concerns about using automated tools for participation assessment? If yes, please explain.

   _______________________________________________________________________

---

### SECTION 6 — Post-Study Participation & Follow-Up Consent

1. **Are you willing to be contacted for a short follow-up interview related to this study?**
   - Yes
   - No

2. **If yes, please indicate your preferred date or general availability for a 30-minute interview:**

   Answer: _______________

3. **Are you willing to try an early prototype or demo of ENGAGIUM once available?**
   - Yes
   - No

4. **If you answered yes, please provide an email address for the prototype testing invitation:**

   Answer: _______________

5. **Would you like to receive a notification once ENGAGIUM's beta version is released for faculty testing?**
   - Yes
   - No

---

## F.2 Item Summary by Construct

The following table summarizes all Likert-scale items organized by construct:

| Construct | Code | # Items | Scale |
|-----------|------|---------|-------|
| **Problem Validation** | PV | 10 | 5-point Likert |
| **Perceived Usefulness** | PU | 5 | 5-point Likert |
| **Perceived Ease of Use** | PEOU | 5 | 5-point Likert |
| **Behavioral Intention** | BI | 5 | 5-point Likert |
| **Feasibility Constraints** | FC | 5 | 5-point Likert |
| **Dashboard & Interface Expectations** | DIE | 5 | 5-point Likert |
| **Report Preferences** | RP | 5 | 5-point Likert |
| **Tracking Options** | TO | 5 | 5-point Likert |
| **Data Privacy Expectations** | DP | 5 | 5-point Likert |
| **Total Likert Items** | — | **50** | — |

**Notes on Item Development:**
- TAM constructs (PU, PEOU, BI) are adapted from Davis (1989) and Venkatesh et al. (2003)
- Problem Validation items are researcher-made based on literature review
- Usability expectation items are researcher-made based on system requirements
- Data Privacy items are researcher-made based on local regulations and ethical guidelines

---

## F.3 Post-Development Survey Instrument

> **[PLACEHOLDER]**
>
> This section will contain the post-development evaluation instruments to be administered after system implementation and testing. The following instruments are planned:
>
> **F.3.1 System Usability Scale (SUS)**
> - 10-item standardized usability questionnaire
> - Adapted from Brooke (1996)
>
> **F.3.2 Post-Deployment TAM Evaluation**
> - Perceived Usefulness (actual use)
> - Perceived Ease of Use (actual use)
> - Behavioral Intention (continued use)
>
> **F.3.3 Accuracy Feedback Questionnaire**
> - User perception of detection accuracy
> - False positive/negative observations
> - Feature reliability ratings
>
> **F.3.4 Open-Ended Feedback**
> - Strengths observed during use
> - Areas for improvement
> - Feature requests
>
> *To be developed and administered during the closed beta testing phase.*
