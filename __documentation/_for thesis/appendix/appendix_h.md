# APPENDIX H  
EVALUATION RESULTS

This appendix presents the results of the evaluation activities conducted for the ENGAGIUM system, including survey responses, usability scores, and accuracy testing.

> **Note:** This appendix contains template tables with placeholder values `[TBD]`. Actual values will be inserted upon completion of data collection and analysis.

---

## H.1 Survey Results Table

### H.1.1 Pre-Development Survey: Problem Validation

**Construct:** Current Practices and Difficulties  
**Respondents:** N = [TBD]

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| PV1 | I find it difficult to monitor all forms of student participation during online classes. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV2 | I often fail to notice quiet students who participate through chat. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV3 | It is challenging to track microphone participation while lecturing. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV4 | It is easy to miss student reactions (e.g., raise hand, thumbs up, emojis). | [TBD] | [TBD] | [TBD] | [TBD] |
| PV5 | Manual participation tracking affects my focus while teaching. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV6 | Recording participation manually takes significant time and effort. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV7 | My participation grading may sometimes appear subjective or inconsistent. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV8 | Students sometimes question how participation grades were determined. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV9 | I believe there is a need for a more structured and objective way to track participation. | [TBD] | [TBD] | [TBD] | [TBD] |
| PV10 | I find it challenging to maintain consistent participation records across multiple sessions. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall** | | | **[TBD]** | **[TBD]** | **[TBD]** |

---

### H.1.2 Pre-Development Survey: Technology Acceptance (TAM)

#### Perceived Usefulness (PU)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| PU1 | A tool that automatically tracks participation would make grading easier. | [TBD] | [TBD] | [TBD] | [TBD] |
| PU2 | Automated participation tracking would increase fairness in evaluating students. | [TBD] | [TBD] | [TBD] | [TBD] |
| PU3 | A system like ENGAGIUM would help ensure that all forms of participation are acknowledged. | [TBD] | [TBD] | [TBD] | [TBD] |
| PU4 | Automated analytics would help me better understand class engagement trends. | [TBD] | [TBD] | [TBD] | [TBD] |
| PU5 | A tracking system would reduce errors caused by manual monitoring. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall PU** | | | **[TBD]** | **[TBD]** | **[TBD]** |

#### Perceived Ease of Use (PEOU)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| PEOU1 | I am comfortable using browser-based tools or extensions. | [TBD] | [TBD] | [TBD] | [TBD] |
| PEOU2 | I can easily learn new digital platforms when needed. | [TBD] | [TBD] | [TBD] | [TBD] |
| PEOU3 | A dashboard for participation analytics would be easy for me to understand. | [TBD] | [TBD] | [TBD] | [TBD] |
| PEOU4 | I prefer tools that require minimal setup or configuration. | [TBD] | [TBD] | [TBD] | [TBD] |
| PEOU5 | I believe I can use an automated tracking tool without extensive training. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall PEOU** | | | **[TBD]** | **[TBD]** | **[TBD]** |

#### Behavioral Intention (BI)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| BI1 | I am willing to use a participation-tracking tool if it is accurate. | [TBD] | [TBD] | [TBD] | [TBD] |
| BI2 | I will adopt a tool like ENGAGIUM if it reduces manual workload. | [TBD] | [TBD] | [TBD] | [TBD] |
| BI3 | I am likely to integrate automated tracking into my online class workflow. | [TBD] | [TBD] | [TBD] | [TBD] |
| BI4 | I am open to adopting new instructional technologies that improve teaching efficiency. | [TBD] | [TBD] | [TBD] | [TBD] |
| BI5 | I am willing to regularly use a tool like ENGAGIUM if it proves reliable. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall BI** | | | **[TBD]** | **[TBD]** | **[TBD]** |

#### Feasibility Constraints (FC)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| FC1 | My device is capable of running additional browser extensions. | [TBD] | [TBD] | [TBD] | [TBD] |
| FC2 | My internet connection is generally stable enough for online tools. | [TBD] | [TBD] | [TBD] | [TBD] |
| FC3 | I do not foresee major barriers in using an automated tracking system. | [TBD] | [TBD] | [TBD] | [TBD] |
| FC4 | I have access to updated browsers or software needed for such tools. | [TBD] | [TBD] | [TBD] | [TBD] |
| FC5 | I have sufficient familiarity with online teaching platforms to integrate an additional tool. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall FC** | | | **[TBD]** | **[TBD]** | **[TBD]** |

---

### H.1.3 Pre-Development Survey: Usability Expectations

#### Dashboard & Interface Expectations (DIE)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| DIE1 | I prefer a simple and clean dashboard layout. | [TBD] | [TBD] | [TBD] | [TBD] |
| DIE2 | I want the dashboard to show participation summaries at a glance. | [TBD] | [TBD] | [TBD] | [TBD] |
| DIE3 | I prefer having graphs and charts to visualize engagement. | [TBD] | [TBD] | [TBD] | [TBD] |
| DIE4 | I want the system to allow filtering by date, class, or participation type. | [TBD] | [TBD] | [TBD] | [TBD] |
| DIE5 | I want real-time indicators showing which participation types are active. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall DIE** | | | **[TBD]** | **[TBD]** | **[TBD]** |

#### Report Preferences (RP)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| RP1 | I prefer downloadable participation reports (e.g., PDF, CSV). | [TBD] | [TBD] | [TBD] | [TBD] |
| RP2 | I want reports to contain totals, frequencies, and participation types. | [TBD] | [TBD] | [TBD] | [TBD] |
| RP3 | I prefer reports that highlight the top participants. | [TBD] | [TBD] | [TBD] | [TBD] |
| RP4 | I prefer reports that identify students who rarely participate. | [TBD] | [TBD] | [TBD] | [TBD] |
| RP5 | I want participation reports to show session-by-session engagement comparisons. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall RP** | | | **[TBD]** | **[TBD]** | **[TBD]** |

#### Tracking Options (TO)

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| TO1 | I want the ability to enable or disable specific participation types. | [TBD] | [TBD] | [TBD] | [TBD] |
| TO2 | I prefer having customizable weighting for speaking, chat, and reactions. | [TBD] | [TBD] | [TBD] | [TBD] |
| TO3 | I want the option to manually adjust participation scores if needed. | [TBD] | [TBD] | [TBD] | [TBD] |
| TO4 | I expect the system to notify me when tracking starts or stops. | [TBD] | [TBD] | [TBD] | [TBD] |
| TO5 | I want the system to allow correction or deletion of incorrect participation logs. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall TO** | | | **[TBD]** | **[TBD]** | **[TBD]** |

---

### H.1.4 Pre-Development Survey: Data Privacy Expectations

| Code | Item | N | Mean | SD | Interpretation |
|------|------|---|------|----|--------------  |
| DP1 | I expect the system to store only necessary participation data. | [TBD] | [TBD] | [TBD] | [TBD] |
| DP2 | I need assurance that no audio or video recordings will be stored. | [TBD] | [TBD] | [TBD] | [TBD] |
| DP3 | I expect clear transparency on how participation is scored and interpreted. | [TBD] | [TBD] | [TBD] | [TBD] |
| DP4 | I consider privacy compliance (e.g., Data Privacy Act of 2012) important for adopting such a system. | [TBD] | [TBD] | [TBD] | [TBD] |
| DP5 | I expect the system to include clear consent procedures before collecting participation data. | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall DP** | | | **[TBD]** | **[TBD]** | **[TBD]** |

---

## H.2 TAM Construct Summaries

### H.2.1 Pre-Development TAM Results Summary

| Construct | # Items | N | Mean | SD | Interpretation |
|-----------|---------|---|------|----|--------------  |
| Perceived Usefulness (PU) | 5 | [TBD] | [TBD] | [TBD] | [TBD] |
| Perceived Ease of Use (PEOU) | 5 | [TBD] | [TBD] | [TBD] | [TBD] |
| Behavioral Intention (BI) | 5 | [TBD] | [TBD] | [TBD] | [TBD] |
| Feasibility Constraints (FC) | 5 | [TBD] | [TBD] | [TBD] | [TBD] |

**Interpretation Guide (per Appendix G.1):**

| Range | Interpretation |
|-------|----------------|
| 4.21 – 5.00 | Very High / Strongly Agree |
| 3.41 – 4.20 | High / Agree |
| 2.61 – 3.40 | Moderate / Neutral |
| 1.81 – 2.60 | Low / Disagree |
| 1.00 – 1.80 | Very Low / Strongly Disagree |

**Analysis:**

[TBD - Narrative interpretation of TAM results will be added after data collection]

---

### H.2.2 Post-Development TAM Results Summary

> **[PLACEHOLDER]**
>
> Post-development TAM evaluation results will be inserted after closed beta testing is completed.

| Construct | # Items | N | Mean | SD | Interpretation |
|-----------|---------|---|------|----|--------------  |
| Perceived Usefulness (PU) | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Perceived Ease of Use (PEOU) | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Behavioral Intention (BI) | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |

---

## H.3 SUS Score Summary

> **[PLACEHOLDER]**
>
> System Usability Scale (SUS) results will be inserted after post-development evaluation.

### H.3.1 SUS Score Distribution

| Statistic | Value |
|-----------|-------|
| N (Respondents) | [TBD] |
| Mean SUS Score | [TBD] |
| Standard Deviation | [TBD] |
| Minimum Score | [TBD] |
| Maximum Score | [TBD] |
| Median Score | [TBD] |

### H.3.2 SUS Score Interpretation

| SUS Score Range | Grade | Adjective Rating | Percentile Range |
|-----------------|-------|------------------|------------------|
| 84.1 – 100 | A+ | Best Imaginable | 96 – 100 |
| 80.8 – 84.0 | A | Excellent | 90 – 95 |
| 78.9 – 80.7 | A- | — | 85 – 89 |
| 77.2 – 78.8 | B+ | — | 80 – 84 |
| 74.1 – 77.1 | B | Good | 70 – 79 |
| 72.6 – 74.0 | B- | — | 65 – 69 |
| 71.1 – 72.5 | C+ | — | 60 – 64 |
| 65.0 – 71.0 | C | OK | 41 – 59 |
| 62.7 – 64.9 | C- | — | 35 – 40 |
| 51.7 – 62.6 | D | Poor | 15 – 34 |
| 0 – 51.6 | F | Worst Imaginable | 0 – 14 |

*(Based on Bangor et al., 2008; Sauro & Lewis, 2016)*

**ENGAGIUM SUS Result:** [TBD]

**Interpretation:**

[TBD - Narrative interpretation comparing to benchmark score of 68 will be added after data collection]

---

## H.4 Accuracy Testing Results

> **[PLACEHOLDER]**
>
> Accuracy testing results will be inserted after controlled testing sessions are completed.

### H.4.1 Detection Accuracy by Event Type

Testing was conducted using [TBD] controlled sessions with [TBD] known events.

| Event Type | True Positives (TP) | False Positives (FP) | False Negatives (FN) | Precision | Recall | F1 Score |
|------------|---------------------|----------------------|----------------------|-----------|--------|----------|
| Participant Join | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Participant Leave | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Chat Message | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Reaction | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Hand Raise | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| Mic Unmute | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] | [TBD] |
| **Overall** | **[TBD]** | **[TBD]** | **[TBD]** | **[TBD]** | **[TBD]** | **[TBD]** |

### H.4.2 Accuracy Metrics Formulas

*(Refer to Appendix G.3 for detailed formulas)*

$$\text{Precision} = \frac{TP}{TP + FP}$$

$$\text{Recall} = \frac{TP}{TP + FN}$$

$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

### H.4.3 Accuracy Interpretation

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| Overall Precision | ≥ 0.85 | [TBD] | [TBD] |
| Overall Recall | ≥ 0.85 | [TBD] | [TBD] |
| Overall F1 Score | ≥ 0.85 | [TBD] | [TBD] |

**Analysis:**

[TBD - Narrative analysis of accuracy results will be added after testing]

---

## H.5 Thematic Analysis Summary

> **[PLACEHOLDER]**
>
> Thematic analysis of open-ended responses will be inserted after qualitative data analysis.

### H.5.1 Pre-Development Open-Ended Themes

Based on analysis of [TBD] responses to open-ended questions:

| Theme | Description | Frequency | Example Quote |
|-------|-------------|-----------|---------------|
| [TBD] | [TBD] | [TBD] | "[TBD]" |
| [TBD] | [TBD] | [TBD] | "[TBD]" |
| [TBD] | [TBD] | [TBD] | "[TBD]" |
| [TBD] | [TBD] | [TBD] | "[TBD]" |
| [TBD] | [TBD] | [TBD] | "[TBD]" |

### H.5.2 Post-Development Open-Ended Themes

> **[PLACEHOLDER]**
>
> To be completed after post-development evaluation.

| Theme | Description | Frequency | Example Quote |
|-------|-------------|-----------|---------------|
| [TBD] | [TBD] | [TBD] | "[TBD]" |
| [TBD] | [TBD] | [TBD] | "[TBD]" |
| [TBD] | [TBD] | [TBD] | "[TBD]" |

---

## H.6 Reliability Analysis

> **[PLACEHOLDER]**
>
> Cronbach's alpha values for each construct will be computed and inserted after data collection.

### H.6.1 Internal Consistency (Cronbach's Alpha)

| Construct | # Items | Cronbach's α | Interpretation |
|-----------|---------|--------------|----------------|
| Problem Validation (PV) | 10 | [TBD] | [TBD] |
| Perceived Usefulness (PU) | 5 | [TBD] | [TBD] |
| Perceived Ease of Use (PEOU) | 5 | [TBD] | [TBD] |
| Behavioral Intention (BI) | 5 | [TBD] | [TBD] |
| Feasibility Constraints (FC) | 5 | [TBD] | [TBD] |
| Dashboard & Interface Exp. (DIE) | 5 | [TBD] | [TBD] |
| Report Preferences (RP) | 5 | [TBD] | [TBD] |
| Tracking Options (TO) | 5 | [TBD] | [TBD] |
| Data Privacy Exp. (DP) | 5 | [TBD] | [TBD] |

**Interpretation Guide (per Appendix G.4):**

| α Value | Interpretation |
|---------|----------------|
| ≥ 0.90 | Excellent |
| 0.80 – 0.89 | Good |
| 0.70 – 0.79 | Acceptable |
| 0.60 – 0.69 | Questionable |
| 0.50 – 0.59 | Poor |
| < 0.50 | Unacceptable |
